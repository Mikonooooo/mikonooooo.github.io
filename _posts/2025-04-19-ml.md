---
title: A lense on machine learning for the general public
category: Computer Science
---

We will talk about 4 important points on a high level:
1. What is Machine Learning?
2. Why is it so hot right now?
3. What makes a good Machine Learning algorithm?
4. What are the ethical consequences of Machine Learning? <!-- more --> 

#### What is machine learning?

Say we are programming a computer to play Super Mario Bros. How would we tell it to complete the game? Here's one way: _Run to the right, when see Goomba jump, when hit wall jump, when see question block jump._ It's very tedious. Instead, machine learning attempts to give the computer the ability to learn like a human. 

![SMB 3]({{ site.baseurl }}/assets/images/2025-04-19-ml/mario.png){: width="400"}

What makes it easy to learn for a human? **Humans learn from trial and error.** Death after death, we can learn how to maneuver around obstacles, figure out what is dangerous and what it not. **Humans learn from observation.** We can watch our siblings, friends, and streamer play the game and learn from their demonstrations. Finally, **humans learn from habit.** There's some part of our brain from genetics and past experience that tells us it's good to collect coins and not run into the Goombas. 

Following [this](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) tutorial, we can train a machine learning model on human demonstrations to play Super Mario Bros: 

![RL agent playing Super Mario Bros.]({{ site.baseurl }}/assets/images/2025-04-19-ml/mario.gif)

In all, **Machine Learning** gives the computer the ability to learn like a human.

#### Why is it so hot right now?

![Timeline of AI/ML waves]({{ site.baseurl }}/assets/images/2025-04-19-ml/AI_ML_winters.png){: width="600"}

The popularity of Machine Learning has waxed and waned from the 1940s to early 2000s. It has regained popularity in academia in the last 13 years, and popularity with the public in the last 2 years. 

Beginning around 2010, we started building Machine Learning-based models that were deeper and bigger. In 2012, performance in the [ImageNet](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/) competition shot up and surpassed the threshold of what researcher thought was possible for any computer program. Researchers started to get excited about the capabilities of **deep learning**.

In 2016, Google DeepMind's [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/) beats Lee Sedol, one of the best players in Go in a match. In 2022, OpenAI launches [ChatGPT](https://openai.com/index/chatgpt/), a Large-Language Model (LLM) that processes almost every natural language query. ChatGPT gained over 100 million users in 2 months. See below for an example. Following closely are rivals like [Gemini](https://gemini.google.com/app), [LLama](https://www.llama.com/), [Claude](https://www.anthropic.com/claude), and recently [DeepSeek](https://www.deepseek.com/en).

![ChatGPT example prompt and response.]({{ site.baseurl }}/assets/images/2025-04-19-ml/gpt.png){: width="600"}

How have Machine Learning-based algorithms become so good? You might have heard that these models are getting bigger, are training on more data, and are quite expensive to train. But is that it? See the following xkcd comic.

![xkcd comic on machine learning]({{ site.baseurl }}/assets/images/2025-04-19-ml/machine_learning_xkcd.png){: width="400"}

It shows a common misconception that Machine Learning is all about throwing the latest resources together and seeing if it works. Some part of that is true, but the connotation is that Machine Learning scientists are blind monkeys at work. **I want to break this assumption** and introduce a principled way we can reason about Machine Learning.

#### What makes a good Machine Learning algorithm?
Recall the setting of training a model to play Super Mario Bros. We may have a series of human demonstrations and train a model to replicate that behavior.

![Training Super Mario Bros]({{ site.baseurl }}/assets/images/2025-04-19-ml/training_mario.png){: width="900"}

How can we measure the success of the algorithm? When we let the trained model play the game, we want the computer to complete the levels, avoid death, keep moving to the right, and collect coins and power-ups.

In Machine Learning, we typically consider how terribly the model performs in the real world, and we call that the **true error**. So the goal of Machine Learning is to produce a model that minimizes the true error.

We will now introduce the key principle for reasoning about machine, the **bias-variance decomposition**:

$$\textsf{true error} = \mathsf{bias} + \mathsf{variance}$$

**Bias** is the absolute best error we can achieve if our Machine Learning algorithm is given _unlimited_ data, memory, and compute. In the real world, it's not possible to have unlimited data, memory, and compute. So **variance** is the variation you usually have from the best error due to the limited data, memory, and compute. Typically, bias and variance have a tradeoff: as you increase one you decrease the other. 

![bias-variance tradeoff]({{ site.baseurl }}/assets/images/2025-04-19-ml/bv-tradeoff.png){: width="500"}

Interpret the diagram above there the model complexity increases along the $$x$$-axis and the true error (called total error here) varies with the $$y$$-axis. As model complexity increases, the bias decreases because that complex model can learn how to solve a difficult tasks given enough data, memory, and compute. However, variance increases because it's hard to to find the exact optimal model when it is so complex.

As model complexity decreases, the bias increases because the model is not powerful enough to solve a difficult task. However, variance decreases because the model is smaller and easier to train to find the optimal model.

#### ChatGPT: A Case Study

Why is ChatGPT such a good chat bot? GPT-4 is around 1 trillion parameters. This is an inconceivable amount. In the 1990s we were studying neural networks with a few hundred parameters, and in the 2010s with a few million parameters. If ChatGPT is so complex, then that explains why bias is so low. But isn't variance large, and wouldn't that make the true error large?

The trick is that we can lower the variance curve. First, ChatGPT is trained on a lot of data, essentially the entire internet. on text across the entire internet. This alleviates the data bottleneck in the variance. Further, ChatGPT takes advantage of a transformer-based architecture that is very efficient when used with Graphics Processing Units (GPUs) and GPT-4 trained over several months, costing over 100 million U.S. dollars. This allieviates the compute and memory-bottleneck in the variance. Thus ChatGPT pushes the variance curve down to look like this:

![bias-variance tradeoff of ChatGPT]({{ site.baseurl }}/assets/images/2025-04-19-ml/bias-variance-gpt.png){: width="750"}

Why did OpenAI spend millions of dollars training its LLMs? Because they knew it would work. They found reliable **scaling laws** that showed a predictable decrease in true error given that model size, data, and compute power all increases.

Now we know that Machine Learning is powerful, and a bit of the intuition how we got there. Now we consider the ethics of Machine Learning.

#### Will AI take over the world?

The good news is that AI is not smarter than us. It can make stupid mistakes. LLMs [hallucinates](https://www.evidentlyai.com/blog/llm-hallucination-examples) not infrequently. Self-driving cars can [miss](https://www.traffictechnologytoday.com/news/autonomous-vehicles/waymo-issues-recall-for-672-autonomous-vehicles-after-pole-collision.html) obstacles. ChatGPT [can't draw](https://www.youtube.com/watch?v=160F8F8mXlo) a glass half-full of wine. At the end of the day, these models are just computer programs.

The bad new is that AI doesn't have to be smarter than us to harm us. Use of AI in critical systems like autonomous vehicles or [adaptive cancer treatment](https://www.moffitt.org/newsroom/news-releases/moffitt-study-shows-ai-boosts-efficacy-of-cancer-treatment-but-doctors-remain-key) has serious consequences if errors occur.  Google search results for "unprofessional haircuts" [have returned](http://theguardian.com/technology/2016/apr/08/does-google-unprofessional-hair-results-prove-algorithms-racist-) a disproportionate amount of black women with natural haircuts. Facial recognition algorithms [struggle to recognize](https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/) black faces. Amazon had a hiring algorithm that was [biased against](https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html) women. AI-based algorithms have been used in the US criminal justice system [to argue](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/) the likelihood of a person committing the crime again. In 2024, Google's Gemini [tried to unbias](https://reason.com/2024/05/28/the-great-black-pope-and-asian-nazi-debacle-of-2024/) its image generation which lead to unprompted generation of historically inaccurate figures like a Black pope or an Asian Nazi.

![Unbiased-biased Gemini]({{ site.baseurl }}/assets/images/2025-04-19-ml/unbias-gemini.png){: width="600"}

Finally, many companies use ML-based targeting to figure out what videos, shorts, reels, stories, and ads they should recommend to keep you using their application. The result is that we don't get content we necessarily want, but often content that provokes us either through fear, anxiety, or dopamine rushes, and sends us into a doomscrolling rabbit hole. These algorithms have been blamed on elevating misinformation and [heighten](https://www.npr.org/2023/07/27/1190383104/new-study-shows-just-how-facebooks-algorithm-shapes-conservative-and-liberal-bub) political polarization.

We have seen the power and the dangers of machine learning. Let's take a step back, and understand what makes a machine learning algorithm.


#### Summary
Machine Learning gives the computer the ability to learn from the data without being explicitly programmed. While has become extremely popular and powerful, we have to be careful about fairness, bias and how we use ML. Finally, the bias-variance decomposition gives us a principled way to evaluate machine learning algorithms. 

_**Acknowledgements**: the first half of the article is taken directly from [CS 4780 Introduction to Machine Learning](https://www.cs.cornell.edu/courses/cs4780/2023fa/){: target="_blank"}. The history and certain areas are inspired by [CS 4782 Introduction to Deep Learning](https://www.cs.cornell.edu/courses/cs4782/2025sp/){: target="_blank"}._

_**Links**: This article is adapted from a class for Splash @ Cornell. Follow along with slides [here]({{ site.baseurl }}/assets/slides/how-to-reason-ml.pdf){: target="_blank"}._ See original version of article [here]({{ site.baseurl }}/page/understanding-machine-learning/).