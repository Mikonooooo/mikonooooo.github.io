---
title: How to reason about machine learning 
category: Computer Science
---

What is machine learning? Say we are programming a computer to play Super Mario Bros. How would we tell it to complete the game? It would be tedious to program the inputs line-by-line. Machine learning (ML) makes this easy. <!-- more --> 

We collect many examples of human demonstrations, run an ML algorithm, and out comes a model, like a [neural network](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.64978&showTestData=false&discretize=false&percTrainData=50&x=true&y=false&xTimesY=false&xSquared=true&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) which can take in frames of the game and produce inputs to the controller to play for us. Following [this](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) tutorial, we get something like this:

![RL agent playing Super Mario Bros.]({{ site.baseurl }}/assets/images/mario.gif)

**Machine learning** gives the computer the ability to learn from data without explicit programming.

![xkcd comic on machine learning]({{ site.baseurl }}/assets/images/machine_learning_xkcd.png){: width="400"}

Machine learning not about pouring large amount of compute and data into a black box and stirring it around. We can reason about it principally.

#### A brief history of ML

There have been 3 periods of growth for AI/ML. In the 1960s there was a lot of excitement about the [perceptron](https://medium.com/@aliceheimanxyz/the-perceptron-explained-9bf9183c4999) (1-layer neural network) for machine translation. But it didn't get anywhere close to translating languages. And no one knew how to train multiple-layer neural networks. So came the first AI winter.

![Timeline of AI/ML waves]({{ site.baseurl }}/assets/images/AI_ML_winters.png){: width="600"}

Then in the 1980s, [backpropogation](https://xnought.github.io/backprop-explainer/) was discovered and allowed us to train larger, multi-layer neural networks. But still the models didn't do that well. Then came the second AI winter. 

Finally, in the 2010s, we started taking advantage of GPUs and started developing tools for deep learning, for neural networks with hundreds and thousands of layers. In 2012, performance in the [ImageNet](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/) competition shot up, and researchers took notice. In 2016, Google DeepMind's [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/) beats Lee Sedol, one of the best players in Go in a match. In 2019, OpenAI's [robot hand](https://openai.com/index/solving-rubiks-cube/) learns how to solve a Rubik's cube without any instructions. In 2022, OpenAI launches [ChatGPT](https://openai.com/index/chatgpt/), a Large-Language Model (LLM) that processes almost every natural language query. ChatGPT gained over 100 million users in 2 months. Following closely are rivals like [Gemini](https://gemini.google.com/app), [LLama](https://www.llama.com/), [Claude](https://www.anthropic.com/claude), and recently [DeepSeek](https://www.deepseek.com/en). OpenAI's [DALLÂ·E 3](https://openai.com/index/dall-e-3/) can generate images you ask for, like this one of "potato kings".

![Timeline of AI/ML waves]({{ site.baseurl }}/assets/images/potatoking.webp){: width="600"}

ML-based AI is immensely powerful, and there's a lot of things it will enable us to do. But its power also means it's dangerous, and we must be careful.s

#### Will AI take over the world?

The good news is that AI is not smarter than us. It can make stupid mistakes. LLMs [hallucinates](https://www.evidentlyai.com/blog/llm-hallucination-examples) not infrequently. Self-driving cars can [miss](https://www.traffictechnologytoday.com/news/autonomous-vehicles/waymo-issues-recall-for-672-autonomous-vehicles-after-pole-collision.html) obstacles. ChatGPT [can't draw](https://www.youtube.com/watch?v=160F8F8mXlo) a glass half-full of wine. At the end of the day, these models are just computer programs.

The bad new is that AI doesn't have to be smarter than us to harm us. Use of AI in critical systems like autonomous vehicles or [adaptive cancer treatment](https://www.moffitt.org/newsroom/news-releases/moffitt-study-shows-ai-boosts-efficacy-of-cancer-treatment-but-doctors-remain-key) has serious consequences if errors occur.  Google search results for "unprofessional haircuts" [have returned](http://theguardian.com/technology/2016/apr/08/does-google-unprofessional-hair-results-prove-algorithms-racist-) a disproportionate amount of black women with natural haircuts. Facial recognition algorithms [struggle to recognize](https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/) black faces. Amazon had a hiring algorithm that was [biased against](https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html) women. AI-based algorithms have been used in the US criminal justice system [to argue](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/) the likelihood of a person committing the crime again. In 2024, Google's Gemini [tried to unbias](https://reason.com/2024/05/28/the-great-black-pope-and-asian-nazi-debacle-of-2024/) its image generation which lead to unprompted generation of historically inaccurate figures like a Black pope or an Asian Nazi.

![Unbiased-biased Gemini]({{ site.baseurl }}/assets/images/unbias-gemini.png){: width="600"}

Finally, many companies use ML-based targeting to figure out what videos, shorts, reels, stories, and ads they should recommend to keep you using their application. The result is that we don't get content we necessarily want, but often content that provokes us either through fear, anxiety, or dopamine rushes, and sends us into a doomscrolling rabbit hole. These algorithms have been blamed on elevating misinformation and [heighten](https://www.npr.org/2023/07/27/1190383104/new-study-shows-just-how-facebooks-algorithm-shapes-conservative-and-liberal-bub) political polarization.

We have seen the power and the dangers of machine learning. Let's take a step back, and understand what makes a machine learning algorithm.

## How to reason about machine learning
Recall the setting of training a neural network to play Super Mario Bros.

![Training Super Mario Bros]({{ site.baseurl }}/assets/images/training_mario.png){: width="900"}

The goal of the ML algorithm is to minimize the **true error** of the model, how terribly the model performs in the real world. Here, the true error is how far is Mario from the flagpole. To reason about the true error, we use the **bias-variance decomposition**:

$$\textsf{true error} = \mathsf{bias} + \mathsf{variance}$$

**Bias** is the absolute best test error that our ML algorithm can obtain if it has _unlimited_ data and compute power. **Variance** is the variation you usually have from the best test error because we only have limited data and compute power. To see this in action, let's take the following example.

#### Learning to finish sentences

The task is make an ML algorithm that takes in a training set of completed sentences, and produces a model that can finish any sentence. See example below.

![Learning to finish sentences]({{ site.baseurl }}/assets/images/learning_sentences.png){: width="800"}

The true error is the number of partial sentences which the model cannot finish grammatically. _Note that this is exactly how ChatGPT responds to your queries! It just tries to fill in the answer to your question._

#### The bi-gram model

For a first analysis, we introduce the **bi-gram** model. The ML algorithm converts each sentence in the training set to pairs of consecutive words called bi-grams.

![converting sentences to bi-grams]({{ site.baseurl }}/assets/images/bi_gram_training.png){: width="800"}

Then the model predicts the next word in the sentence by using the most frequent bi-gram that matches on the last word. So if we gave as input "I like...", then the model outputs "... to drink coffee...". The model has two choices after "coffee", let's break the tie arbitrarily: "... has caffeine.". So in all, "I like to drink coffee has caffeine.". 

The sentences sounds uncanny since it makes sense when you look at pairs of words, but it's not grammatical. Most sentences will be nonsensical like this, so bi-grams give high true error. The bias is very large. English is complex and a model cannot finish a sentence by only looking at the last word. Thus, no matter how many sentences we train on, the bi-gram model will perform terribly. On the other hand, variance is small because if you look at consecutive words in English, you won't find much variation. After "doctor's" we can expect "note" or "office" or "orders", and pretty much nothing else.

#### The memorization model

We introduce the **memorization model**. The training data is not processed at all, instead the model checks if its input matches any sentences in the training data, and if anything matches it finishes the sentence in exactly the same way. 

For example, on input "We like to...", the model outputs "..drink coffee here!". On the other hand, on almost every other input like "We're tired, so we drink...", the model doesn't find a match and outputs nothing.

We cannot possibly memorize most sentences (there are an infinite number of sentences), so memorization also gives a high true error. Oddly though, bias is small. If we somehow trained on all possible english sentences and have infinite memory and compute power, then memorization performs perfectly. The key was, in reality, we have finite data and memory, so the variance from the optimal model is very high. 

#### Bias-variance tradeoff

Both models had high true error, the bi-gram model had high bias and small variance while memorization had high variance and small bias. We observe a tradeoff between the bias and variance of a model. See diagram below.

![bias-variance tradeoff]({{ site.baseurl }}/assets/images/bias-variance.png){: width="500"}

So the optimal model is somewhere in-between the complexity of bi-grams versus memorization. Notice that ChatGPT is placed slightly to the left of memorization. GPT-4 has an unconceivable 1 trillion parameters, so it's complex. But ChatGPT can finish sentences with ease. Why doesn't it suffer from high variance?

#### How ChatGPT cheats
The trick is that OpenAI pushes the variance curve down. They lower variance by training their natural language tasks on the entire internet. ChatGPT uses [transformers](https://www.datacamp.com/tutorial/how-transformers-work), which take advantage of GPUs to do thousands of operations in parallel. Finally, OpenAI spent ~100 million USD and several months to train GPT-4. By breaking data-based and computational barriers, ChatGPT gets the benefits of both low bias and variance. The tradeoff looks more like the following.

![bias-variance tradeoff of ChatGPT]({{ site.baseurl }}/assets/images/bias-variance-gpt.png){: width="800"}

OpenAI found this reliable, positive relationship between model performance increasing the training data, number of model parameters, and compute power, and called it **[scaling laws](https://medium.com/@lukelimdy/openais-research-paper-scaling-laws-for-neural-language-models-key-takeaways-d177438553d8){: target="_blank"}**.

#### Summary
Machine Learning gives the computer the ability to learn from the data without being explicitly programmed. While has become extremely popular and powerful, we have to be careful about fairness, bias and how we use ML. Finally, the bias-variance decomposition gives us a principled way to evaluate machine learning algorithms. 

_**Acknowledgements**: the first half of the article is taken directly from [CS 4780 Introduction to Machine Learning](https://www.cs.cornell.edu/courses/cs4780/2023fa/){: target="_blank"}. The history and certain areas are inspired by [CS 4782 Introduction to Deep Learning](https://www.cs.cornell.edu/courses/cs4782/2025sp/){: target="_blank"}._

_**Links**: This article is adapted from a class for Splash @ Cornell. Follow along with slides [here]({{ site.baseurl }}/assets/slides/Intro_to_ML.pdf){: target="_blank"}._ See original version of article [here]({{ site.baseurl }}/page/understanding-machine-learning/).