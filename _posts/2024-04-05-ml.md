---
title: Understanding Machine Learning
category: Computer Science
---

What is even learning? Learning is applying past and current experience and using it to make and informed prediction about some information we do not already know. For example, human learn how to answers questions on the final exam, or predict how crabby we will feel after staying up all night, or learn when they've had too much to drink.<!-- more -->

#### Rats vs Pigeons

To help us characterize learning, let's see some real world examples. Rats exhibit **bait shyness**, a trait that lets rats learn to avoid poisonous foods. Given food of a new look or smell, rats eat will eat in very small quantities. If they experience sickness, they stop eating it. They learn to associate the sickness with the look and smell of the food. Learning is very useful for survival.

We contrast this against the pigeon. In an experiment by B.F. Skinner, pigeons were fed at variable times independent of the behavior of the pigeons. But somehow the pigeons started repeating the same action (pecking or turning their head) over and over again. We'll call this **pigeon superstition**. The pigeons learned a nonsensical association between their behavior and receiving food.  Now, learning is just wasting energy. 

#### Generalization & Inductive Bias

When is learning useful and when it is useless? Let's define some terms. **Generalization** is how well we make the right prediction in the real world based on past and current experiences. **Inductive bias** are the assumptions we make on how we make the prediction (i.e. what's the algorithm we follow).

The rats try to predict what food is poisonous or not. The pigeons try to predict what action brings food. We see that the rat's bait shyness generalizes better than the pigeon's superstitions. So what biological mechanisms let this happen? If we revisit the rats, repeating the experiment replacing poison for electrical shocks, the rats don't learn at all which foods lead to electrical shocks. There's biological instinct that tells the rats to only focus on the nauseous feelings, and in this case, nature is right. In contrast, the pigeons do not have such a biological instinct and confuse pecking and turning with receiving food. So the rats have a high inductive bias.

|              |  **inductive bias** | **generalization** |
| **rats**     |        high         |        good        |
| **pigeons**  |        low          |        bad         |

It is not always the case generalization positively correlates with inductive bias. For example, if the rats snuck into someone's house and started eating their food, the home owners might find out and exterminate the rats. The rat's bait shyness was not enough to generalize to human kitchens.

And, for example, if the pigeons were being fed by a machine that rewarded them for repeatedly pecking or nodding, then the pigeons would be generalizing well.

#### Machine Learning

Why do we need a machine to learn? We are interested in automating tasks that humans would normally do. For example, answer general questions, flag spam emails, and read the text captured in a photo. We are also interested in doing tasks that humans can't do easily. For example, generating videos and images from prompts, detecting anomalies in big data, predicting how different proteins fold. 

There are many paradigms for machine learning. Today, we will study what's known as **supervised statistical batch learning with a passive learner**. That means the learner is given a training dataset of past inputs and results, and then must predict given new inputs. I broke this already when talking about rats and pigeons. The rats and pigeons are **supervised statistical online learning with an active learner**. I leave it as an exercise for the reader to decipher that. As we will see, the discussion about generalization vs inductive bias is just the tip of the iceberg. To reason about this more, we're going to have to get specific... and formal.

#### A Formal Setting with Papayas

We are stranded on a tropical island where papayas are regularly eaten. We've never seen a papaya before, so we are not sure how to tell if a papaya is yummy or not. We decide that we will judge the yumminess based on the color, which ranges from dark green to orange, red and dark brown, and the softness ranging from solid to mushy. We buy a sample of papayas from the market to try out. Our goal is to learn what makes a yummy papaya in this market. Our first step is to come up with a formal model for this situation.

First, let's call the features of papaya $$i$$ the **input features** $$\vec x_i=(x_{\mathsf{color}}, x_{\mathsf{softness}})$$. Say we can measure softness and color over the range over $$[0,1]$$. We call the **feature space** $$\mathcal{X} \triangleq [0,1]^2$$. So $$\vec x_i \in \mathcal{X}$$. We may model $$\mathcal X$$ as a unit square and $$\vec x_i$$ is  point in the square. We can also use papaya $$i$$ and $$\vec x_i$$ interchangeably.

Let's call the papaya $$i$$'s yumminess the **label** $$y_i$$ where $$y_i=1$$ if it is yummy and $$y_i=-1$$ if it is not. We call the **label space** $$\mathcal Y\triangleq \{\pm 1\}$$, and $$y_i\in\mathcal Y$$.

Say we have $$m$$ papayas. Our small sample of papayas is our **training set** $$D\triangleq ((\vec x_1,y_1),\ldots (\vec x_m, y_m))$$. 

Suppose we have a learning algorithm $$A$$. Given $$D$$, it outputs a **hypothesis** $$h:\mathcal X\to\mathcal Y$$, a function that is able to predict if a papaya is yummy based on its color and softness. We denote $$h\triangleq A(D)$$. We say $$h\in \mathcal H$$ for some **hypothesis space** $$\mathcal H$$ which we will not worry about right now.

Finally, we need a way for evaluate if $$h$$ predicts yumminess well. We use a **loss function** $$L: \mathcal Y\times  \mathcal Y \to [0,1]$$ where $$L(\hat y, y)=0$$ if $$\hat y= y$$ and $$L(\hat y, y)=1$$ if $$\hat y\neq y$$. This loss function detects mismatches between the predicted label $$\hat y$$  and the true label $$y$$. To obtain the loss of a hypothesis $$h$$ over a set $$S=((\vec x_1,y_1),\ldots,(\vec x_n,y_n))$$, we define the **empirical loss of $$h$$ over $$S$$**:

$$
\begin{gather*}
    L_{h}(\vec x, y) \triangleq L(h(\vec x), y) \\
    L_{h}(S) \triangleq \frac{1}{n} \sum_{i=1}^n L(h(\vec x_i), y_i).
\end{gather*}
$$

Notice the empirical loss counts the proportion of misclassifications $$h$$ has on $$S$$. We can define the **training loss** $$L_h(D)$$. But how do know if $$h$$ generalizes well? We can define the **generalization loss** too. If $$\mathbb D$$ is the set of all papayas in the market, we have $$L_h(\mathbb D)$$. 

#### Learning with Axis-Aligned Papayas

While our initial definition that learning strives to minimize $$L_h(\mathbb D)$$, we need to get more precise. Let's investigate this by attempting to characterize a concrete algorithm. We restrict $$\mathcal H_{\mathsf{rect}}$$ to the set of axis-aligned rectangular classifiers, that is if $$h\in\mathcal H_{\mathsf{rect}}$$ there is a rectangle $$R$$ in $$[0,1]^2$$ for which

$$
\begin{gather*}
    h(\vec x) = \begin{cases}+1 & x \in R \\ -1 & x\notin R\end{cases}.
\end{gather*}
$$

Let's try to see how we would implement an algorithm $$A_{\mathsf{rect}}$$ that tries to find an $$h\in\mathcal H_{\mathsf{rect}}$$ that generalizes well. Given $$D$$ as input, we draw the smallest rectangle around all the yummy papayas in $$D$$. We can see that it's possible to code up this algorithm. What's the problem?

This algorithm, $$A_{\mathsf{rect}}$$ does not always work. With some non-zero probability, $$D$$ could be not even close to looking uniform. Suppose that all the yummy and non-yummy papayas are clumped in $$\alpha\times\alpha$$ squares for small $$\alpha>0$$. Then we would draw an $$\alpha\times\alpha$$ square around the yummy papayas, and $$L_h(D)=0$$ while $$L_h(\mathbb D) = 0.5 - \alpha^2\approx 0.5$$.

To combat this, we can increase the size of the dataset, but the above can still happen with very small probability. We need to adjust our success parameters to say that we achieve good generalization _on average_. One way to enforce this is to introduce a small **confidence parameter** $$\delta>0$$ where we want $$A_{\mathsf{rect}}$$ to return an $$h\in \mathcal H_{\mathsf{rect}}$$ that minimizes the generalization loss with probability at least $$1-\delta$$. So $$L_{h}(\mathcal D)=0$$ with probability $$1-\delta$$. 

While we know that there exists an axis-aligned rectangle that achieves $$0$$ generalization loss, can our algorithm actually find it? If $$D$$ has a vague decision boundary, this will be impossible. To account for this, we add a small **error parameter** $$\varepsilon>0$$ where we want $$L_h(\mathbb D) \leq \varepsilon$$.

We can make this slightly more general. For axis-aligned rectangles, there exists an $$h\in\mathcal H_{\mathsf{rect}}$$ which achieves $$0$$ generalization loss. In general, this may not be the case. (e.g. we take a circle hypothesis space.) If no hypothesis perfectly generalizes, then no matter what's our algorithm, the outputted hypothesis will have loss lower bounded by $$\min_{h\in\mathcal H} L_h(\mathbb D)$$ which is greater than $$0$$. Now our approximation error guarantee fits $$L_h(\mathcal D) \leq \min_{h\in\mathcal H} L_h(\mathbb D) + \varepsilon$$. 

We can now state the proper definition of learning. **Probably approximately correct learning** (PAC learning) is defined as follows. An algorithm $$A$$ is an $$(\varepsilon,\delta)$$-PAC learner if over the randomness of selecting $$D\subseteq\mathbb D$$ of size $$m$$ and $$h=A_{\mathsf{rect}}(D)$$, then $$L_h(\mathbb D) \leq \varepsilon$$ with probability at least $$1-\delta$$.

Finally, recall that $$m$$ needs to be sufficiently large so $$D$$ is representative of $$\mathbb D$$. We call $$m(\varepsilon, \delta)$$ the **sample complexity** if it is the smallest size of $$D$$ to maintain $$(\varepsilon,\delta)$$-PAC learning.

#### Analyzing Axis-Aligned Papayas

For what sample complexity is $$A_{\mathsf{rect}}$$ an $$(0.01,0.1)$$-PAC learner? In other words, how many papayas do we need to buy for training so that with $$90\%$$ confidence, we only misclassify $$1\%$$ of all papayas? This is rather tricky, we will not go over the solution, though feel free to check it by clicking the dropdown below.

<details class="dropdown">
  <summary class="dropdown__summary">
    See solution.
  </summary>
  <div class="dropdown__content" markdown="1">
Call $$Y$$ the inner square and $$R$$ the rectangle $$h=A_{\mathsf{rect}}(D)$$ classifies on. Notice $$R$$ is always inside $$Y$$. Then $$L_h(\mathbb D)=\frac{1}{2} - \mathsf{area}(R)$$. So it is sufficient to show that the area of $$R$$ is greater than $$\frac{1}{2} - 0.01$$ with probability at least $$0.9$$.

INSERT IMAGE

Let $$Y'$$ be a centered square inside $$Y$$ of area $$\frac{1}{2}-0.01$$. If $$Y'$$ is inside in $$R$$ too, then the area of $$R$$ is at least $$\frac{1}{2} - 0.01$$. So it is sufficient to show that $$Y'$$ is inside of $$R$$ with probability at least $$0.9$$.

Consider the 4 thin margins between $$Y$$ and $$Y'$$ along the 4 sides. Define four events $$E_1$$, $$E_2$$, $$E_3$$, $$E_4$$ where $$E_i$$ is the event were all $$m$$ papayas do _not_ fall in strip $$i$$. Notice that if none of the $$E_i$$ occur, then there exists a papaya in each strip, so $$R$$ must be large enough that $$Y'$$ is inside $$R$$. 

Since each papaya is sampled independently, uniformly at random from $$[0,1]^2$$, and the area of each strip is about $$0.01/4 = 0.0025$$, the probability each papaya falls outside of that is $$1-0.0025=0.9975$$. Then the probability of $$E_i$$, that all $$m$$ papayas fall outside of the strip $$i$$, is $$(0.9975)^{m}$$. If we choose $$m=1474$$, $$(0.9975)^{1474} < 0.025.$$

The probability that at least one of the $$E_i$$ occurs can be at most the sum of the probability of each $$E_i$$ by itself. So, when $$m=1474$$, that probability is less than $$0.025 \cdot 4 = 0.1$$. Therefore, the probability none of the $$E_i$$ occur is at least $$0.9$$. Thus, $$Y'$$ is inside $$R$$ with probability at least $$0.9$$. So the sample complexity is $$m=1474$$.
  </div>
</details>

So $$m=1474$$. That's a lot of papayas. If we settled for $$10\%$$ error rate, we only need $$m=148$$. That's still a lot. Okay, we cheated a bit. We only found an upper bound on the sample complexity, and it's pretty bad.

 Coincidence that the sample complexity when down by a factor of $$10$$? Nope, the sample complexity for the general case $$(\varepsilon,\delta)$$-PAC learning is $$m(\varepsilon,\delta)= \frac{\sqrt 2 \ln({4}/{\delta})}{\varepsilon}.$$ The analysis is a bit out of scope of this article. If you are interested, see section 1.4.2 of [these](https://www.cs.cornell.edu/courses/cs4850/2025sp/handouts/randalgs.pdf) lectures notes.

#### When axis-aligned rectangles don't work

It was obvious to us that an axis-aligned rectangle was the right shape for the decision boundary, but when does it fail? We've already identified if $$m$$ is too small, the rectangle it finds will be too small. We can also consider if the $$\mathbb D$$ has a different decision boundary. Suppose it is a diamond of area $$1/2$$. Then we won't be able to get close to $$0$$ generalization error with an axis-aligned rectangle. Maybe $$\min_{h\in\mathcal H_{\mathsf{rect}}} L_{h}(\mathbb D_{\mathsf{diam}})=0.25$$. This is called underfitting. It's when the learner can't generalize well. To combat underfitting, we can do the following:

1. Add more training samples. We need a sufficient size to be representative of $$\mathbb D$$.
2. Decrease inductive bias by expanding the hypothesis class. We could consider rotated rectangles which we would get a similar (slightly worse) guarantee as the original problem.
3. Add more features to each papaya. Color and softness may not be enough, maybe we need to factor in smell to properly determine a papaya's yumminess.

All good things in moderation. It can be expensive to expand the training set: it costs money to buy papayas. 

If we add on too many features, the learner may find nonsensical relationships in the training data which don't generalize well. This is known as the **curse of dimensionality**. 

If we expand the hypothesis class too much, we may find hypotheses that have low training loss but high generalization loss. For example, let us expand $$\mathcal H$$ to the set of all functions from $$[0,1]^2$$ to $$\{\pm 1\}$$.

But this is not true because $$h$$ is not independent from $$D$$. Specifically, we can have a hypothesis $$h$$ that memorizes $$D$$ and recalls the correct label for each papaya. So $$L_h(D)=0$$. Let us denote the yummy papayas with a square of area $$1/2$$. Suppose that $$\mathcal D$$ is spread uniformly across $$[0,1]^2$$. If $$m=10$$ but there are $$1,000$$ papayas, we will correctly predict $$10$$ papayas and get roughly $$\frac{990}{2}$$ others correct, so $$L_h(\mathcal D) = 0.495$$. You might as well flip a fair coin.

This phenomenon is called **overfitting**. This is when the model memorizes the training set too well, and so does not generalize well. This happens when the model has low inductive bias and a small amount of training data. To combat overfitting, we can do many things:

1. Increase the size of $$D$$. If $$m=500$$, then for a memorizer $$L_h(\mathcal D) = 0.25$$.
2. Restrict the hypothesis class $$\mathcal H$$. Instead of memorizing, what can $$h$$ do? Some ideas are to look at the nearest neighbor or predict over an axis-aligned rectangles. Then our models would actually do quite well. With sufficient data, our model will find a good decision boundary and have $$L_h(D)=0$$ and $$L_h(\mathbb D)\approx 0$$. We will revist this soon.

It is not always feasible to collect more data. And if we add too much bias, we can get horrendous generalization. If $$\mathcal H$$ is just the set of linear classifiers, no line does a good job at separating yummy from not yummy. The best we can do is $$L_h(\mathcal D)=0.354$$. Funnily though, training loss will be close to generalization loss. This phenomenon is called **underfitting**. This is when the model has high inductive bias and the prediction task is too complex. To combat this we can do these things as well:



If we add too many features, this can be bad too since it could lead the model to find nonsensical associations and overfit. This is called the **curse of dimensionality**. Here's a recap of what we know so far.

|                   |  **inductive bias**   | **training set** | **\# features**   | **generalization**    |  **training loss**   |
| **overfitting**   |         low           | small             | many               |    bad               |  good                 |
| **underfitting**  |        high           | big               | few               |    bad               |  bad                  |
| **just right**    |          medium       |  not too small              | sufficient               |   good               |  either               |

#### Bias-Variance Tradeoff

Let's recap what we know so far. 




#### What the hell is happening with LLMs.

#### Summary

_**Acknowledgements:** The first half is adapted from Chapters 1 & 2 of [Understanding Machine Learning](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) by Shai Shalev-Schwartz and Shai Ben-David. Introduction of PAC Learning is inspired from [CS 4850 Probability, Vectors, and Matrices in Computing](https://www.cs.cornell.edu/courses/cs4850/2025sp/). Discussion on BV Tradeoff are adapted from [CS 4780 Introduction to Machine Learning](https://www.cs.cornell.edu/courses/cs4780/2023fa/). Finally, discussion on LLMs is contributed from [CS 4782 Introduction to Deep Learning](https://www.cs.cornell.edu/courses/cs4782/2025sp/). This article will be adapted for the corresponding Splash @ Cornell for Spring 2025._

_**Image References:** from x, y, z_