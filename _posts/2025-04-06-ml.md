---
title: How to reason about machine learning 
category: Computer Science
---

What is machine learning? Say we are programming a computer to play Super Mario Bros. How would we tell it to complete the game? It would be tedious to program the inputs line-by-line. We want a program that can identify enemies, when to jump and what the the end goal of the level. 

Machine learning (ML) makes this easy. We collect many examples of human demonstrations, run an ML algorithm, and out comes a model, like a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) which can take in frames of the game and produce inputs to the controller to play for us. Following [this](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) tutorial, we get something like this:

![RL agent playing Super Mario Bros.]({{ site.baseurl }}/assets/images/mario.gif)

**Machine learning** gives the computer the ability to learn from data without explicit programming.

![xkcd comic on machine learning]({{ site.baseurl }}/assets/images/machine_learning_xkcd.png){: width="400"}

Machine learning not about pouring large amount of compute and data into a black box and stirring it around. We can reason about it principally.

#### A brief history of ML

There have been 3 periods of growth for AI/ML. In the 1960s there was a lot of excitement about the [perceptron](https://en.wikipedia.org/wiki/Perceptron) (1-layer neural network) for machine translation. But it didn't get anywhere close to translating languages. And no one knew how to train multiple-layer neural networks. So came the first AI winter.

![Timeline of AI/ML waves]({{ site.baseurl }}/assets/images/AI_ML_winters.png){: width="600"}

Then in the 1980s, [backpropogation](https://en.wikipedia.org/wiki/Backpropagation) was discovered and allowed us to train larger, multi-layer neural networks. But still the models didn't do that well. Then came the second AI winter. 

Finally, in the 2010s, we started taking advantage of GPUs and started developing tools for deep learning, for neural networks with hundreds and thousands of layers. In 2012, performance in the [ImageNet](https://en.wikipedia.org/wiki/ImageNet#) competition shot up, and researchers took notice. In 2016, Google DeepMind's [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/) beats Lee Sedol, one of the best players in Go in a match. In 2019, OpenAI's [robot hand](https://openai.com/index/solving-rubiks-cube/) learns how to solve a Rubik's cube without any instructions. In 2022, OpenAI launches [ChatGPT](https://openai.com/index/chatgpt/), a Large-Language Model (LLM) that processes almost every natural language query. ChatGPT gained over 100 million users in 2 months. Following closely are rivals like [Gemini](https://gemini.google.com/app), [LLama](https://www.llama.com/), [Claude](https://www.anthropic.com/claude), and recently [DeepSeek](https://www.deepseek.com/en). OpenAI's DALLÂ·E 3 can generate images you ask for, like this one of "potato kings".

![Timeline of AI/ML waves]({{ site.baseurl }}/assets/images/potatoking.webp){: width="600"}

ML-based AI is immensely powerful, and there's a lot of things it will enable us to do. But it's power also means it's dangerous, and we must be careful.

#### Will AI take over the world?

The good news is that AI is not smarter than us. It can make stupid mistakes. LLMs [hallucinates](https://www.evidentlyai.com/blog/llm-hallucination-examples) not infrequently. Self-driving cars can [miss](https://www.traffictechnologytoday.com/news/autonomous-vehicles/waymo-issues-recall-for-672-autonomous-vehicles-after-pole-collision.html) obstacles. ChatGPT [can't draw](https://www.youtube.com/watch?v=160F8F8mXlo) a glass half-full of wine. At the end of the day, these models are just computer programs.

The bad new is that AI doesn't have to be smarter than us to harm us. Use of AI in critical systems like autonomous vehicles or [adaptive cancer treatment](https://www.moffitt.org/newsroom/news-releases/moffitt-study-shows-ai-boosts-efficacy-of-cancer-treatment-but-doctors-remain-key) has serious consequences if errors occur. 

AI can be biased and unfair. Google search results for "unprofessional haircuts" [have returned](http://theguardian.com/technology/2016/apr/08/does-google-unprofessional-hair-results-prove-algorithms-racist-) a disproportionate amount of black women with natural haircuts. Facial recognition algorithms [struggle to recognize](https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/) black faces. Amazon had a hiring algorithm that was [biased against](https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html) women. AI-based algorithms have been used in the US criminal justice system [to argue](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/) the likelihood of a person committing the crime again.

AI is very good at finding correlations in the data. But correlations is not causation. So, especially for the last point about AI-based criminal risk scoring, the scores will be necessarily biased against those who already face systemic bias in the criminal justice system like Blacks and Latinos.

Finally, many companies use ML-based targeting to figure out what videos, shorts, reels, stories, and ads they should recommend to keep you using their application. The result is that we don't get content we necessarily want, but often content that provokes us and sends us into a doomscrolling rabbit hole. The marketing algorithms analyze your habits, prey on fear and anxiety, and have been found to elevate misinformation and [heighten](https://www.npr.org/2023/07/27/1190383104/new-study-shows-just-how-facebooks-algorithm-shapes-conservative-and-liberal-bub) political polarization.

We have seen the power and the dangers of machine learning. Let's take a step back, and understand what makes a machine learning algorithm.

## How to reason about machine learning

_**Acknowledgements**: the first half of the article is taken directly from Wen Sun's version of CS 4780 Introduction to Machine Learning._